
#' Simulation of cluster randomized trial with contamination
#'
#' \code{simulateCRT} generates simulated data for a cluster randomized trial (CRT) with geographic contamination between arms.
#'
#' @param trial an object of class \code{"CRTsp"} or a data frame containing locations in (x,y) coordinates, cluster
#'   assignments (factor \code{cluster}), and arm assignments (factor \code{arm}). Each location may also be
#'   assigned a \code{propensity} (see details).
#' @param effect numeric. The simulated effect size (defaults to 0)
#' @param outcome0 numeric. The anticipated value of the outcome in the absence of intervention
#' @param generateBaseline logical. If \code{TRUE} then baseline data and the \code{propensity} will be simulated
#' @param matchedPair logical. If \code{TRUE} then the function tries to carry out randomization
#' using pair-matching on the baseline data (see details)
#' @param scale measurement scale of the outcome. Options are: 'proportion' (the default); 'count'; 'continuous'.
#' @param baselineNumerator optional name of numerator variable for pre-existing baseline data
#' @param baselineDenominator optional name of denominator variable for pre-existing baseline data
#' @param denominator optional name of denominator variable for the outcome
#' @param kernels number of kernels used to generate a de novo \code{propensity}
#' @param ICC_inp numeric. Target intra cluster correlation, provided as input when baseline data are to be simulated
#' @param sd numeric. standard deviation of the normal kernel measuring spatial smoothing leading to contamination
#' @param theta_inp numeric. input contamination range
#' @param tol numeric. tolerance of output ICC
#' @returns A list of class \code{"CRTsp"} containing the following components:
#' \tabular{lll}{
#' \code{geom_full}\tab list: \tab summary statistics describing the site
#' cluster assignments, and randomization \cr
#' \code{design}\tab list: \tab values of input parameters to the design \cr
#' \code{trial} \tab data frame: \tab rows correspond to geolocated points, as follows:\cr
#' \tab \code{x} \tab numeric vector:  x-coordinates of locations \cr
#' \tab \code{y} \tab numeric vector:  y-coordinates of locations \cr
#' \tab\code{cluster} \tab factor:  assignments to cluster of each location  \cr
#' \tab\code{arm} \tab factor:  assignments to \code{control} or \code{intervention} for each location \cr
#' \tab\code{nearestDiscord} \tab numeric vector:  signed Euclidean distance to nearest discordant location (km) \cr
#' \tab\code{propensity} \tab numeric vector:  propensity for each location \cr
#' \tab\code{base_denom} \tab numeric vector:  denominator for baseline \cr
#' \tab\code{base_num} \tab numeric vector:  numerator for baseline \cr
#' \tab\code{denom} \tab numeric vector:  denominator for the outcome \cr
#' \tab\code{num} \tab numeric vector:  numerator for the outcome \cr
#' \tab\code{...} \tab other objects included in the input \code{"CRTsp"} object
#' or \code{data.frame}\cr
#' }
#' @details Synthetic data are generated by sampling around the values of
#' variable \code{propensity}, which is a numerical vector
#' (taking positive values) of length equal to the number of locations.
#' There are three ways in which \code{propensity} can arise:
#' \enumerate{
#' \item \code{propensity} can be provided as part of the input \code{trial} object.
#' \item Baseline numerators and denominators (values of \code{baselineNumerator}
#' and \code{baselineDenominator} may be provided.
#' \code{propensity} is then generated as the numerator:denominator ratio
#' for each location in the input object
#' \item Otherwise \code{propensity} is generated using a 2D Normal
#' kernel density. The [\code{OOR::StoSOO}](https://rdrr.io/cran/OOR/man/StoSOO.html)
#' is used to achieve an intra-cluster correlation coefficient (ICC) that approximates
#' the value of \code{'ICC_inp'} by searching for an appropriate value of the kernel bandwidth.
#' }
#' \code{num[i]}, the synthetic outcome for location \code{i}
#' is simulated with expectation: \cr
#' \deqn{E(num[i]) = outcome0[i] * propensity[i] * denom[i] * (1 - effect*I[i])/mean(outcome0[] * propensity[])} \cr
#' The sampling distribution of \code{num[i]} depends on the value of \code{scale} as follows: \cr
#' - \code{scale}=’continuous’: Values of \code{num} are sampled from a
#' Normal distributions with means \code{E(num[i])}
#' and variance determined by the fitting to \code{ICC_inp}.\cr
#' - \code{scale}=’count’: Simulated events are allocated to locations via multivariate hypergeometric distributions
#' parameterised with \code{E(num[i])}.\cr
#' - \code{scale}=’proportion’: Simulated events are allocated to locations via multinomial distributions
#' parameterised with \code{E(num[i])}.\cr
#'
#' \code{denominator} may specify a vector of numeric (non-zero) values
#' in the input \code{"CRTsp"} or \code{data.frame} which is returned
#' as variable \code{denom}. It acts as a scale-factor for continuous outcomes, rate-multiplier
#' for counts, or denominator for proportions. For discrete data all values of \code{denom}
#' must be > 0.5 and are rounded to the nearest integer in calculations of \code{num}.\cr\cr
#' By default, \code{denom} is generated as a vector of ones, leading to simulation of
#' dichotomous outcomes if \code{scale}=’proportion’.\cr
#'
#' If baseline numerators and denominators are provided then the output vectors
#' \code{base_denom} and  \code{base_num} are set to the input values. If baseline numerators and denominators
#' are not provided then the synthetic baseline data are generated by sampling around \code{propensity} in the same
#' way as the outcome data, but with the effect size set to zero.
#'
#' If \code{matchedPair} is \code{TRUE} then pair-matching on the baseline data will be used in randomization providing
#' there are an even number of clusters. If there are an odd number of clusters then matched pairs are not generated and
#' an unmatched randomization is output.
#'
#' Either \code{sd} or \code{theta_inp} must be provided. If both are provided then
#' the value of \code{sd} is overwritten
#' by the standard deviation implicit in the value of \code{theta_inp}.
#' Contamination is simulated as arising from a diffusion-like process.
#'
#' For further details see [Multerer (2021)](https://edoc.unibas.ch/85228/)
#' @export
#'
#' @examples
#' {smalltrial <- readdata('smalltrial.csv')
#'  simulation <- simulateCRT(smalltrial,
#'   effect = 0.25,
#'   ICC_inp = 0.05,
#'   outcome0 = 0.5,
#'   matchedPair = FALSE,
#'   scale = 'proportion',
#'   sd = 0.6,
#'   tol = 0.05)
#'  summary(simulation)
#'  }
simulateCRT <- function(trial = NULL, effect = 0, outcome0 = NULL, generateBaseline = TRUE, matchedPair = TRUE,
                        scale = "proportion", baselineNumerator = "base_num", baselineDenominator = "base_denom",
                        denominator = NULL, ICC_inp = NULL, kernels = 200, sd = NULL, theta_inp = NULL, tol = 5e-03) {

    # Written by Tom Smith, July 2017. Adapted by Lea Multerer, September 2017
    message("\n=====================    SIMULATION OF CLUSTER RANDOMISED TRIAL    =================\n")
    bw <- NULL
    if (!is.null(trial)) CRT <- CRTsp(trial)
    trial <- CRT$trial

    if (is.null(trial$cluster)){
      message("*** Clusters not yet assigned ***")
      return()
    }
    trial$cluster <- as.factor(trial$cluster)
    if (is.null(trial$arm)){
        message("*** No randomization available ***")
        return()
    }
    trial$arm <- as.factor(trial$arm)


    # set the denominator variable to be 'denom'
    if (is.null(denominator)) denominator <- "denom"
        trial$denom <- trial[[denominator]]
    if (denominator != "denom") trial[[denominator]] <- NULL

    # If the denominator has no values set, set to one
    if (is.null(trial$denom)) trial$denom <- 1

    # use contamination range if this is available
    if (!is.null(theta_inp)) {
        sd <- theta_inp/(2 * qnorm(0.975))
    }
    if (is.null(sd)) {
        stop("Contamination range or s.d. of contamination must be provided")
    }

    # compute distances to nearest discordant locations if they do not exist
    if (is.null(trial$nearestDiscord)) trial <- compute_distance(trial, distance = "nearestDiscord")$trial

    # trial needs to be ordered for GEE analyses (estimation of ICC)
    trial <- trial[order(trial$cluster), ]

    # remove any pre-existing propensity if a new one is required
    if (generateBaseline) trial$propensity <- NULL

    if (is.null(trial$propensity)){
      # If baseline numerator data exist and propensity doesn't
        if (baselineNumerator %in% colnames(trial)){
          if (!(baselineDenominator %in% colnames(trial))){
            trial[[baselineDenominator]] <- 1
          }
          # expand the vector of locations to allow for denominators > 1
          trial <- dplyr::mutate(trial, kernelID = dplyr::row_number())
          kernels  <- tidyr::uncount(trial, trial[[baselineNumerator]])
          kernels$weight <- 1/kernels[[baselineDenominator]]
          # sample the kernels if there are a lot (> 100) of them
          if(nrow(kernels) > 100){
             includedkernels <- sample(x = nrow(kernels), size = 100, replace = FALSE)
             kernels <- kernels[includedkernels,c("x","y","weight")]
          }
          # force bandwidth to be scalar
          bandwidth = 0.3
          xdist <- with(trial, outer(x, kernels$x, "-"))
          ydist <- with(trial, outer(y, kernels$y, "-"))
          # distance matrix between the locations and the kernel midpoints
          euclid <- sqrt(xdist * xdist + ydist * ydist)

          # 2d normal kernels
          f <- (1/(2 * pi * bandwidth^2)) * exp(-(euclid^2)/(2 * (bandwidth^2)))
          totalf <- f %*% kernels$weight   #weighted sums over rows of matrix f (weighted by denominators)

          # Scale propensity so that it is between zero and one
          trial$propensity <- totalf/max(totalf)

        } else {
          # If propensity doesn't exist and there is no baseline
          # create initial proposal for propensity (this will be smoothed)
          # bw = 0.3 and kernels = 50 aims to ensure propensity is not very close to zero in sites a few
          # km to tens of km across
          trial$propensity <- createPropensity(trial, bandwidth = 0.3, kernels = kernels)
        }
    }


    # For the smoothing step compute contributions to the relative effect size from other locations as a function of
    # distance to the other locations

    euclid <- distance_matrix(trial$x, trial$y)

    # compute approximate diagonal of clusters

    approx_diag <- sqrt((max(trial$x) - min(trial$x))^2 + (max(trial$y) -
                          min(trial$y))^2)/sqrt(length(unique(trial$cluster)))

    if (is.null(ICC_inp)) {
      stop("*** A target ICC must be specified ***")
    }
    message("Estimating the smoothing required to achieve the target ICC of ", ICC_inp)

    # determine the required smoothing bandwidth by fitting to the pre-specified ICC
    # random multiplier is used to prevent the random number stream being reset to the same value for any given bw value
    random_multiplier <- sample(1e6, size = 1)
    loss <- 999
    nb_iter <- 20
    if (identical(Sys.getenv("TESTTHAT"), "true")) nb_iter <- 5
    # in testing, the number of iterations is reduced giving very approximate output
    while (loss > tol) {
        ICC.loss <- OOR::StoSOO(par = c(NA), fn = ICCdeviation, lower = -5, upper = 5, nb_iter = nb_iter,
                                trial = trial, ICC_inp = ICC_inp, approx_diag = approx_diag, sd = sd,
                                scale = scale, euclid = euclid, effect = effect, outcome0 = outcome0,
                                random_multiplier = random_multiplier)
        loss <- ICC.loss$value
    }
    logbw <- ICC.loss$par
    # overprint the output that was recording progress
    message("\r                                                         \n")

    # recover the seed that generated the best fitting trial and use this to regenerate this trial
    bw <- exp(logbw[1])
    set.seed(round(bw * random_multiplier))

    trial <- get_assignments(trial = trial, scale = scale, euclid = euclid, sd = sd, effect = effect,
                    outcome0 = outcome0, bw = bw, numerator = "num", denominator = "denom")
    # create a baseline dataset using the optimized bandwidth
    if (generateBaseline) trial <- get_assignments(trial = trial, scale = scale,
                    euclid = euclid, sd = sd, effect = 0,
                    outcome0 = outcome0, bw = bw, numerator = baselineNumerator,
                    denominator = baselineDenominator)

    CRT$trial <- trial
    return(CRTsp(CRT))
}

# Assign expected outcome to each location assuming a fixed effect size.
get_assignments <- function(trial, scale, euclid, sd, effect, outcome0,
                            bw = bw, numerator, denominator) {
  expected_ratio <- num <- rowno <- sumnum <- NULL
  # remove any superseded numerator variable
  trial[[numerator]] <- NULL

  # Smooth the propensity.  the s.d. in each dimension of the 2 d gaussian is bw/sqrt(2)
  smoothedPropensity <- gauss(bw, euclid) %*% trial$propensity

  # adjustedpropensity is the value of propensity decremented by the effect of intervention and smoothed
  # by applying a further kernel smoothing step (trap the case with no contamination)
  if (sd < 0.001) sd <- 0.001
  decrementedPropensity <- smoothedPropensity * (1 - effect * (trial$arm == "intervention"))
  adjustedPropensity <- gauss(sd*sqrt(2), euclid) %*% decrementedPropensity

  if (identical(scale, "continuous")) {
    # Note that the sd here is logically different from the smoothing sd, but how to choose a value?
    trial$num <- rnorm(n = nrow(trial),
                       mean = adjustedPropensity * trial[[denominator]],
                       sd = sd)
  } else {
    # compute the total positives expected given the input effect size
    npositives <- round(outcome0 * sum(trial[[denominator]]) * (1 - 0.5 * effect))

    if (!(denominator %in% colnames(trial))) trial[[denominator]] <- 1

    # the denominator must be an integer; this changes the value if a non-integral value is input
    trial[[denominator]] <- round(trial[[denominator]], digits = 0)

    # scale to input value of initial prevalence by assigning required number of infections with probabilities proportional
    # to smoothed multiplied by the denominator

    expected_allocation <-  adjustedPropensity * trial[[denominator]]/sum(adjustedPropensity * trial[[denominator]])

    trial$expected_ratio <- expected_allocation/trial[[denominator]]
    trial$rowno <- seq(1:nrow(trial))

    # expand the vector of locations to allow for denominators > 1
    triallong <- trial %>%
      tidyr::uncount(trial[[denominator]])

    # To generate count data, records in triallong can be sampled multiple times. To generate proportions each record can
    # only be sampled once.
    replacement <- identical(scale, "count")

    # sample generates a multinomial sample and outputs the indices of the locations assigned
    positives <- sample(x = nrow(triallong), size = npositives, replace = replacement, prob = triallong$expected_ratio)
    triallong$num <- 0

    # TODO: this needs to work also for count data (replace = TRUE above)
    triallong$num[positives] <- 1

    # summarise the numerator values into the original set of locations
    numdf <- dplyr::group_by(triallong, rowno) %>%
      dplyr::summarise(sumnum = sum(num))
    numdf[[numerator]] <- numdf$sumnum

    # use left_join to merge into the original data frame (records with zero denominator do not appear in numdf)
    trial <- trial %>%
      dplyr::left_join(numdf, by = "rowno")

    # remove temporary variables and replace missing numerators with zero (because the multinomial sampling algorithm leaves
    # NA values where no events are assigned)
    trial <- subset(trial, select = -c(rowno, expected_ratio, sumnum))
    if (sum(is.na(trial[[numerator]])) > 0) {
      warning("*** Some records have zero denominator after rounding ***")
      message("You may want to remove these records or rescale the denominators")
      trial[[numerator]][is.na(trial[[numerator]])] <- 0
    }
  }
return(trial)
}

# deviation of ICC from target as a function of bandwidth
ICCdeviation <- function(logbw, trial, ICC_inp, approx_diag, sd, scale, euclid, effect, outcome0, random_multiplier) {
    cluster <- NULL
    # set the seed so that a reproducible result is obtained for a specific bandwidth
    if (!is.null(logbw)) {
        bw <- exp(logbw[1])
        set.seed(round(bw * random_multiplier))
    }

    trial <- get_assignments(trial = trial, scale = scale, euclid = euclid, sd = sd, effect = effect,
                             outcome0 = outcome0, bw = bw, numerator = "num", denominator = "denom")
    trial$y1 <- trial$num
    trial$y_off <- trial$denom
    trial$y0 <- trial$denom - trial$num
    link <- map_scale_to_link(scale)

    model_object <- get_GEEmodel(trial = trial, link = link, fterms = 'arm')

    summary.fit <- summary(model_object)
    # Intracluster correlation
    ICC <- noLabels(summary.fit$corr[1])  #with corstr = 'exchangeable', alpha is the ICC
    # message("\rbandwidth: ", bw, "  ICC=", ICC, "        \r")
    loss <- (ICC - ICC_inp)^2
    return(loss)
}


# compute a euclidian distance matrix
distance_matrix <- function(x, y) {
    # generates square matrices of differences
    xdist <- outer(x, x, "-")
    ydist <- outer(y, y, "-")
    euclid <- sqrt(xdist * xdist + ydist * ydist)
}


# add lognormal noise: not sure this function is needed X is the input vector comprising a sample from a smoothed
# distribution varXY is the required variance
add_noise <- function(X, varXY) {
    muY <- 1
    varY <- (varXY - var(X))/(1 + var(X))
    mu <- log(muY/sqrt(1 + varY/muY))
    var <- log(1 + varY/muY)
    Y <- stats::rlnorm(length(XY), meanlog = mu, sdlog = sqrt(var))
    XY <- X * Y
    return(XY)
}


# contribution of i to j as a function of the Gaussian process used in simulating contamination
gauss <- function(bw, euclid) {
    # definition of a gauss function
    f <- (1/(2 * pi * bw^2)) * exp(-(euclid^2)/(2 * (bw^2)))
    totalf <- rowSums(f)  #sums over rows of matrix f
    # Careful here, totalf sums over very small numbers, consider replacing
    return(f/totalf)
}

# generate a de novo pattern of propensity
createPropensity <- function(trial, bandwidth, kernels) {

    # force bandwidth to be scalar
    bandwidth <- bandwidth[1]

    sam <- sample(1:nrow(trial), kernels, replace = TRUE)

    xdist <- with(trial, outer(x, x[sam], "-"))
    ydist <- with(trial, outer(y, y[sam], "-"))
    euclid <- sqrt(xdist * xdist + ydist * ydist)  #is not a square matrix

    f <- (1/(2 * pi * bandwidth^2)) * exp(-(euclid^2)/(2 * (bandwidth^2)))
    totalf <- (1/ncol(euclid)) * rowSums(f)  #sums over rows of matrix f

    # Scale propensity so that it is between zero and one
    propensity <- totalf/max(totalf)

    return(propensity)
}


